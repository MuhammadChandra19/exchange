# Matching Service

A high-performance order matching engine for cryptocurrency exchanges built in Go. This service processes limit and market orders, maintains an orderbook in memory, and generates trade matches with proper price-time priority using FIFO (First In, First Out) matching algorithm.

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Getting Started](#getting-started)
- [Configuration](#configuration)
- [Order Matching Algorithm](#order-matching-algorithm)
- [Performance](#performance)
- [State Management](#state-management)
- [Testing](#testing)
- [Deployment](#deployment)
- [Development](#development)

## Overview

The Matching Service is the heart of a cryptocurrency exchange that:

- **Order Processing**: Handles limit and market orders from users via Kafka
- **Price-Time Priority**: Maintains strict price-time priority with FIFO ordering
- **Real-time Matching**: Instant order matching with sub-millisecond latency
- **Match Publishing**: Publishes trade matches to Kafka for downstream services
- **State Persistence**: Redis-based snapshot system for crash recovery
- **High Throughput**: Optimized for high-frequency trading environments

## Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Order Reader  â”‚    â”‚  Matching Engineâ”‚    â”‚   Orderbook     â”‚
â”‚   (Kafka)       â”‚â”€â”€â”€â–¶â”‚   (Core Logic)  â”‚â”€â”€â”€â–¶â”‚   (In-Memory)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Match Publisher â”‚    â”‚   Snapshot      â”‚
                    â”‚    (Kafka)      â”‚    â”‚   (Redis)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Flow

1. **Order Ingestion**: Orders arrive via Kafka from order management service
2. **Order Processing**: Engine validates and processes orders sequentially
3. **Matching Logic**: Orderbook matches orders using price-time priority
4. **Match Generation**: Successful matches generate trade events
5. **State Persistence**: Periodic snapshots saved to Redis for recovery
6. **Event Publishing**: Match events published to Kafka for downstream processing

### Key Components

- **Engine**: Central coordinator that processes orders and manages components
- **Orderbook**: In-memory data structure maintaining bid/ask limits with FIFO queues
- **Order Reader**: Kafka consumer that ingests orders from upstream services
- **Match Publisher**: Kafka producer that publishes trade matches
- **Snapshot Store**: Redis-based persistence layer for state recovery

## Features

### ğŸš€ **High Performance**
- **In-Memory Orderbook**: Ultra-fast order matching with minimal latency
- **Lock-Free Design**: Optimized concurrency with minimal blocking
- **FIFO Priority**: Fair order execution following price-time priority
- **Batch Processing**: Efficient order processing with batching support

### ğŸ“Š **Trading Features**
- **Limit Orders**: Standard limit orders with price and quantity
- **Market Orders**: Immediate execution at best available prices
- **Order Modification**: Support for order updates and cancellations
- **Partial Fills**: Handles partial order executions efficiently

### ğŸ”„ **Real-time Processing**
- **Kafka Integration**: Asynchronous order processing via message queues
- **Event Streaming**: Real-time match events for downstream services
- **State Recovery**: Crash recovery using Redis snapshots
- **Monitoring**: Built-in metrics and performance monitoring

### ğŸ›  **Production Ready**
- **Graceful Shutdown**: Clean shutdown with proper resource cleanup
- **Error Handling**: Comprehensive error handling and retry mechanisms
- **Configuration**: Flexible environment-based configuration
- **Logging**: Structured logging with configurable levels

## Getting Started

### Prerequisites

- Go 1.22+
- Apache Kafka 3.0+
- Redis 7.0+
- Docker (optional)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd exchange/services/matching-engine
   ```

2. **Install dependencies**
   ```bash
   go mod tidy
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

4. **Start the service**
   ```bash
   go run cmd/main.go
   ```

## Configuration

Configuration is managed through environment variables:

### Required Settings
```env
# Trading pair (required)
PAIR=BTC/USD

# Kafka configuration (required)
KAFKA_TOPIC=orders
KAFKA_BROKER=localhost:9092,localhost:9093
KAFKA_GROUP_ID=matching-engine-btc-usd

# Redis configuration (required)
REDIS_ADDRESS=localhost:6379
REDIS_PASSWORD=
REDIS_USERNAME=
REDIS_DB=0
REDIS_DEFAULT_CHANNEL=exchange
```

### Optional Settings
```env
# Match publisher
MATCH_PUBLISHER_TOPIC=match_events
MATCH_PUBLISHER_BROKER=localhost:9092
```

## Order Matching Algorithm

### Price-Time Priority

The matching engine implements strict **price-time priority** with the following rules:

1. **Price Priority**: Better prices are matched first
   - For buy orders: Higher prices have priority
   - For sell orders: Lower prices have priority

2. **Time Priority**: Among orders at the same price level, earlier orders are matched first (FIFO)

3. **Sequence Priority**: If timestamps are identical, sequence numbers determine priority

### Order Types

#### 1. Limit Orders
```go
type Order struct {
    ID        string
    IsBuy     bool
    Size      float64
    Price     float64
    Timestamp int64
    Sequence  int64
    UserID    string
}
```

**Matching Process:**
- Order is placed at specified price level
- Waits for matching orders on opposite side
- Partial fills possible if counterparty size is smaller

#### 2. Market Orders
```go
// Market order (Price = 0 indicates market order)
marketOrder := &Order{
    ID:        "market_001",
    IsBuy:     true,
    Size:      100.0,
    Price:     0, // Market order indicator
    Timestamp: time.Now().Unix(),
    UserID:    "user_123",
}
```

**Matching Process:**
- Immediately matches against best available prices
- Walks through price levels until filled or no liquidity
- Always executes at counterparty prices (price improvement)

### Matching Algorithm Flow

```go
func (ob *Orderbook) PlaceLimitOrder(order *Order) []Match {
    if order.IsBuy {
        // Try to match against ask side (sell orders)
        matches := ob.matchAgainstAsks(order)
        
        // If order not fully filled, add to bid side
        if order.Size > 0 {
            ob.addToBids(order)
        }
    } else {
        // Try to match against bid side (buy orders)  
        matches := ob.matchAgainstBids(order)
        
        // If order not fully filled, add to ask side
        if order.Size > 0 {
            ob.addToAsks(order)
        }
    }
    return matches
}
```

### Data Structures

#### Orderbook Structure
```go
type Orderbook struct {
    BidLimits map[float64]*Limit  // Price -> Limit (sorted descending)
    AskLimits map[float64]*Limit  // Price -> Limit (sorted ascending)
    mu        sync.RWMutex        // Thread safety
}

type Limit struct {
    Price       float64
    TotalVolume float64
    Orders      []*Order  // FIFO queue
    mu          sync.Mutex
}
```

#### Match Result
```go
type Match struct {
    Ask        *Order   // Sell order
    Bid        *Order   // Buy order  
    SizeFilled float64  // Quantity matched
    Price      float64  // Execution price (limit order price)
}
```

## Performance

### Benchmark Results

The matching engine demonstrates exceptional performance with sub-microsecond order processing:

#### Throughput Analysis (Operations/Second)

| Operation Type | Duration (ns/op) | Throughput (ops/sec) |
|---|---|---|
| State Access (concurrent offset) | 20.49 | **48,804,295** | Ultra High |
| Parallel Market Orders | 868.0 | **1,152,074** | 
| Market Orders (with liquidity) | 1,063 | **940,734** |
| Memory Allocation Test | 1,104 | **905,797** |
| Parallel Limit Orders | 1,199 | **834,028** |
| Single-threaded Limit Orders | 1,211 | **826,010** |
| Mixed Operations (realistic) | 12,803 | **78,119** |
| Small Orderbook Snapshot | 13,674 | **73,128** |
| Large Orderbook Snapshot | 112,457 | **8,892** |

#### Raw Benchmark Data

```
BenchmarkEngine_ProcessLimitOrder/single_threaded_limit_orders-10         	  952934	      1211 ns/op	     742 B/op	       8 allocs/op
BenchmarkEngine_ProcessLimitOrder/parallel_limit_orders-10                	  993500	      1199 ns/op	     743 B/op	       8 allocs/op
BenchmarkEngine_ProcessMarketOrder/market_orders_with_liquidity-10        	 1005352	      1063 ns/op	     654 B/op	       8 allocs/op
BenchmarkEngine_ProcessMarketOrder/parallel_market_orders-10              	 1409472	       868.0 ns/op	     603 B/op	       8 allocs/op
BenchmarkEngine_SnapshotCreation/snapshot_small_orderbook-10              	   82188	     13674 ns/op	   20296 B/op	     116 allocs/op
BenchmarkEngine_SnapshotCreation/snapshot_large_orderbook-10              	    9283	    112457 ns/op	  154551 B/op	    1021 allocs/op
BenchmarkEngine_MixedOperations/mixed_orders_realistic_workload-10        	   93180	     12803 ns/op	    3135 B/op	      15 allocs/op
BenchmarkEngine_StateAccess/concurrent_offset_access-10                   	58417968	        20.49 ns/op	       0 B/op	       0 allocs/op
BenchmarkEngine_MemoryAllocation-10                                       	 1000000	      1104 ns/op	     742 B/op	       8 allocs/op
```

### Memory Efficiency Analysis

#### Core Order Processing (Highly Optimized)
```
Limit Orders:    742-743 B/op,  8 allocs/op
Market Orders:   603-654 B/op,  8 allocs/op  (Most efficient)
Memory Test:     742 B/op,      8 allocs/op
```

**Analysis**: Core order processing is extremely memory-efficient with consistent low allocation patterns.

#### Snapshot Operations (Memory Intensive)
```
Small Orderbook:  20,296 B/op,   116 allocs/op   (277x more memory)
Large Orderbook:  154,551 B/op,  1,021 allocs/op (208x more memory)
Mixed Operations: 3,135 B/op,    15 allocs/op    (4.2x more memory)
```

## State Management

### Snapshot System

The service uses Redis for persistent state management:

#### Snapshot Structure
```go
type Snapshot struct {
    Timestamp    time.Time
    OrderOffset  int64
    BidLimits    map[float64]*Limit
    AskLimits    map[float64]*Limit
    TotalOrders  int64
}
```

#### Snapshot Process
1. **Periodic Snapshots**: Automatic snapshots every N orders or time interval
2. **Redis Storage**: Compressed snapshots stored in Redis with TTL
3. **Recovery**: On startup, load latest snapshot and replay orders since snapshot
4. **Consistency**: Snapshots are atomic and consistent with order processing

#### Recovery Process
```bash
# Service restart recovery flow:
1. Load latest snapshot from Redis
2. Restore orderbook state
3. Resume order processing from last offset
4. Publish recovery completion event
```

## Testing

### Unit Tests
```bash
# Run all tests
go test ./...

# Run tests with coverage
go test -coverprofile=coverage.out ./...
go tool cover -html=coverage.out

# Run benchmarks
go test -bench=. ./...
```

### Integration Tests
```bash
# Run integration tests (requires Kafka & Redis)
go test -tags=integration ./...
```

### Performance Tests
```bash
# Load testing
go test -bench=BenchmarkEngine -benchmem -benchtime=10s ./...
```

### Test Coverage
- **Unit Tests**: >90% coverage
- **Integration Tests**: End-to-end order processing
- **Benchmark Tests**: Performance regression detection
- **Race Condition Tests**: Concurrent access testing

## Deployment

### Docker Deployment
```yaml
version: '3.8'
services:
  matching-engine:
    build: .
    environment:
      - PAIR=BTC/USD
      - KAFKA_BROKER=kafka:9092
      - REDIS_ADDRESS=redis:6379
    depends_on:
      - kafka
      - redis
```

### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: matching-engine-btc-usd
spec:
  replicas: 1  # One instance per trading pair
  selector:
    matchLabels:
      app: matching-engine
      pair: btc-usd
  template:
    spec:
      containers:
      - name: matching-engine
        image: matching-engine:latest
        env:
        - name: PAIR
          value: "BTC/USD"
```

### Production Considerations

- **One Instance per Pair**: Each trading pair requires a dedicated instance
- **Resource Allocation**: CPU-intensive, allocate sufficient CPU cores
- **Memory Requirements**: 1-2GB RAM per instance for orderbook state
- **Network**: Low-latency network for Kafka connectivity
- **Monitoring**: Monitor order processing rate and match generation

## Development

### Project Structure
```
services/matching-engine/
â”œâ”€â”€ cmd/                        # Application entry point
â”‚   â””â”€â”€ main.go                # Service main function
â”œâ”€â”€ internal/                  # Internal packages
â”‚   â”œâ”€â”€ app/                   # Application layer
â”‚   â”‚   â””â”€â”€ engine/           # Core matching engine
â”‚   â”œâ”€â”€ domain/               # Domain layer
â”‚   â”‚   â”œâ”€â”€ match-publisher/  # Match event publishing
â”‚   â”‚   â”œâ”€â”€ order-reader/     # Order consumption
â”‚   â”‚   â”œâ”€â”€ orderbook/       
â”‚   â”‚   â””â”€â”€ snapshot/         # State management
â”‚   â””â”€â”€ usecase/              # Business logic
â”œâ”€â”€ pkg/                      # Public packages
â”‚   â””â”€â”€ config/              # Configuration
â”œâ”€â”€ go.mod                    # Go modules
â””â”€â”€ README.md                # This file
```

### Code Generation
```bash
# Generate mocks
go generate ./...

# Generate protocol buffers (if needed)
buf generate
```

### Development Workflow
1. **Setup**: Clone repo and install dependencies
2. **Testing**: Write tests for new features
3. **Benchmarking**: Verify performance characteristics
4. **Integration**: Test with Kafka and Redis
5. **Documentation**: Update README and code comments

### Contributing
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Write tests for your changes
4. Ensure all tests pass and benchmarks are stable
5. Commit your changes (`git commit -m 'Add amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)  
7. Open a Pull Request

---

**Built with â¤ï¸ for high-frequency trading using Go, Kafka, and Redis**
```

### Directory Structure

services/matching-engine/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ main.go                 # Application entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ engine/             # Main processing engine
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ orderbook/v1/       # Orderbook domain models
â”‚   â”‚   â”œâ”€â”€ order-reader/v1/    # Order reader interface
â”‚   â”‚   â””â”€â”€ snapshot/v1/        # Snapshot domain models
â”‚   â””â”€â”€ usecase/
â”‚       â”œâ”€â”€ orderbook/          # Orderbook implementation
â”‚       â”œâ”€â”€ order-reader/       # Kafka order reader
â”‚       â””â”€â”€ snapshot/           # Redis snapshot store
â”œâ”€â”€ pkg/
â”‚   â””â”€â”€ config/                 # Configuration management
â””â”€â”€ README.md
```

## Features

### Order Processing
- **Limit Orders**: Orders with specific price levels
- **Market Orders**: Orders that execute immediately at best available price
- **Price-Time Priority**: FIFO matching within price levels
- **Partial Fills**: Orders can be partially filled across multiple matches

### Orderbook Management
- **Real-time Updates**: Live orderbook state
- **Price Levels**: Efficient limit management by price
- **Volume Tracking**: Accurate volume calculations
- **Thread Safety**: Concurrent access protection

### State Management
- **Snapshots**: Periodic Redis snapshots for recovery
- **Offset Tracking**: Kafka offset management
- **State Restoration**: Automatic recovery from snapshots

### Performance
- **In-Memory Processing**: Sub-millisecond order processing
- **Efficient Matching**: O(log n) price level operations
- **Minimal Allocations**: Optimized for low latency
- **Concurrent Safe**: Thread-safe operations

## Getting Started

### Prerequisites

- Go 1.21 or later
- Kafka (for order streaming)
- Redis (for state persistence)
- Docker (optional, for containerized deployment)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd exchange/services/matching-engine
   ```

2. **Install dependencies**
   ```bash
   go mod download
   ```

3. **Build the service**
   ```bash
   go build -o bin/matching-engine cmd/main.go
   ```

### Running the Service

1. **Set up configuration**
   ```bash
   export PAIR="BTC-USD"
   export KAFKA_BROKERS="localhost:9092"
   export KAFKA_TOPIC="orders"
   export REDIS_ADDR="localhost:6379"
   ```

2. **Run the service**
   ```bash
   ./bin/matching-engine
   ```

### Docker Deployment

```bash
# Build image
docker build -t matching-engine .

# Run container
docker run -d \
  --name matching-engine \
  -e PAIR=BTC-USD \
  -e KAFKA_BROKERS=kafka:9092 \
  -e REDIS_ADDR=redis:6379 \
  matching-engine
```

## Configuration

### Environment Variables

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `PAIR` | Trading pair (e.g., BTC-USD) | - | Yes |
| `KAFKA_BROKERS` | Kafka broker addresses | `localhost:9092` | Yes |
| `KAFKA_TOPIC` | Kafka topic for orders | `orders` | Yes |
| `REDIS_ADDR` | Redis server address | `localhost:6379` | Yes |
| `REDIS_PASSWORD` | Redis password | - | No |
| `REDIS_DB` | Redis database number | `0` | No |
| `SNAPSHOT_INTERVAL` | Snapshot creation interval | `5m` | No |
| `SNAPSHOT_OFFSET_DELTA` | Orders between snapshots | `1000` | No |

### Configuration File

```yaml
# config.yaml
pair: "BTC-USD"
kafka:
  brokers: ["localhost:9092"]
  topic: "orders"
  groupId: "matching-engine"
redis:
  addrs: "localhost:6379"
  password: ""
  db: 0
snapshot:
  interval: "5m"
  offsetDelta: 1000
```

## API Reference

### Order Format

Orders are consumed from Kafka in JSON format:

```json
{
  "userID": "user123",
  "type": "limit",
  "bid": true,
  "size": 1.5,
  "price": 50000.0
}
```

#### Order Fields

- `userID` (string): User identifier
- `type` (string): Order type - `"limit"` or `"market"`
- `bid` (boolean): `true` for buy orders, `false` for sell orders
- `size` (float64): Order quantity
- `price` (float64): Order price (required for limit orders)

### Match Output

When orders are matched, the service produces matches:

```json
{
  "ask": {
    "id": "order456",
    "userID": "seller123",
    "size": 0.5,
    "bid": false
  },
  "bid": {
    "id": "order789",
    "userID": "buyer456", 
    "size": 0.0,
    "bid": true
  },
  "sizeFilled": 0.5,
  "price": 50000.0
}
```

### Orderbook State

The orderbook maintains the following state:

```go
type Orderbook struct {
    AskLimits map[float64]*Limit  // price -> ask limit
    BidLimits map[float64]*Limit  // price -> bid limit  
    Orders    map[string]*Order   // orderID -> order
}
```

## Testing

### Run All Tests

```bash
go test ./...
```

### Run with Coverage

```bash
go test -cover ./...
```

### Run Specific Tests

```bash
# Test orderbook functionality
go test ./internal/usecase/orderbook -v

# Test limit functionality  
go test ./internal/domain/orderbook/v1 -v

# Test engine
go test ./internal/app/engine -v
```

### Benchmark Tests

```bash
go test -bench=. ./internal/usecase/orderbook
```

## Performance

### Benchmarks

Typical performance on modern hardware:

- **Order Processing**: ~100,000 orders/second
- **Match Generation**: ~50,000 matches/second  
- **Memory Usage**: ~10MB for 100,000 orders
- **Latency**: <1ms for order processing

### Optimization Tips

1. **Batch Processing**: Process multiple orders together
2. **Memory Pools**: Reuse objects to reduce GC pressure
3. **Lock Optimization**: Minimize lock contention
4. **Snapshot Tuning**: Adjust snapshot frequency for your workload

## Monitoring

### Key Metrics

- **Orders Processed**: Total orders processed
- **Matches Generated**: Total matches created
- **Processing Latency**: Order processing time
- **Orderbook Depth**: Number of price levels
- **Memory Usage**: Current memory consumption

### Health Checks

The service exposes health endpoints:

- **Liveness**: `/health/live` - Service is running
- **Readiness**: `/health/ready` - Service is ready to process orders

## Troubleshooting

### Common Issues

1. **Kafka Connection Issues**
   ```
   Error: failed to connect to kafka
   Solution: Check KAFKA_BROKERS configuration
   ```

2. **Redis Connection Issues**
   ```
   Error: failed to connect to redis  
   Solution: Check REDIS_ADDR and credentials
   ```

3. **High Memory Usage**
   ```
   Issue: Memory usage growing continuously
   Solution: Tune snapshot frequency and GC settings
   ```

4. **Slow Processing**
   ```
   Issue: High order processing latency
   Solution: Check for lock contention and optimize orderbook operations
   ```

### Debugging

Enable debug logging:

```bash
export LOG_LEVEL=debug
./bin/matching-engine
```

## Contributing

### Development Setup

1. **Install development tools**
   ```bash
   go install golang.org/x/tools/cmd/goimports@latest
   go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest
   ```

2. **Run linting**
   ```bash
   golangci-lint run
   ```

3. **Format code**
   ```bash
   goimports -w .
   ```

### Code Style

- Follow standard Go conventions
- Use meaningful variable names
- Add comments for public interfaces
- Write tests for new functionality
- Keep functions small and focused

### Pull Request Process

1. Create feature branch from `main`
2. Make changes with tests
3. Run full test suite
4. Submit pull request with description
5. Address review feedback


---

**Note**: This service is designed for high-frequency trading environments. Ensure proper testing and monitoring before production deployment. 